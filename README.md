# Ingest√£o e Busca Sem√¢ntica com LangChain e Postgres

## üéØ Funcionalidades
- **Ingest√£o**: Processar arquivos PDF e armazenar em banco vetorial PostgreSQL
- **Chat**: Interface CLI para perguntas baseadas exclusivamente no conte√∫do do PDF
- **Chat com hist√≥rico**: Interface CLI para perguntas baseadas exclusivamente no conte√∫do do PDF e com hist√≥rico de conversa√ß√£o.

## üìÅ Estrutura do Projeto

```
‚îú‚îÄ‚îÄ docker-compose.yml         # Configura√ß√£o PostgreSQL + pgVector
‚îú‚îÄ‚îÄ requirements.txt           # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env.example              # Template de vari√°veis de ambiente
‚îú‚îÄ‚îÄ libs/
‚îÇ   ‚îî‚îÄ‚îÄ envs/
‚îÇ       ‚îî‚îÄ‚îÄ env_validator.py  # Validador interativo de vari√°veis
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py            # Script de ingest√£o do PDF
‚îÇ   ‚îú‚îÄ‚îÄ search.py            # Script de busca sem√¢ntica
‚îÇ   ‚îú‚îÄ‚îÄ chat.py              # Interface CLI interativa
‚îÇ   ‚îî‚îÄ‚îÄ chat_with_history.py # Interface CLI interativa com hist√≥rico de conversa√ß√£o
‚îú‚îÄ‚îÄ document.pdf             # PDF de exemplo para ingest√£o
‚îî‚îÄ‚îÄ README.md               # Este arquivo
```

## üõ†Ô∏è Tecnologias
- **Python 3.10+** com tipagem
- **LangChain** para processamento de documentos
- **PostgreSQL + pgVector** para armazenamento vetorial
- **Google Gemini** para embeddings e gera√ß√£o de respostas
- **Docker Compose** para infraestrutura

## Pr√©-requisitos
- Python 3.10+
- Docker e Docker Compose
- Chave de API do Google Gemini

## Configura√ß√£o

### 1. Ambiente Virtual
Crie e ative um ambiente virtual:
```bash
python3 -m venv venv
source venv/bin/activate
```

### 2. Depend√™ncias
Instale as depend√™ncias:
```bash
pip install -r requirements.txt
```

### 3. Vari√°veis de Ambiente
Copie o arquivo `.env.example` para `.env` e preencha as vari√°veis:
```bash
cp .env.example .env
```

Edite o arquivo `.env`:
```env
GOOGLE_API_KEY=your_google_api_key_here
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/rag
PG_VECTOR_COLLECTION_NAME=pdf_chunks
PDF_PATH=document.pdf
```

**Importante**: Substitua `your_google_api_key_here` pela sua chave de API do Google Gemini.

## Ordem de Execu√ß√£o

### 1. Subir o banco de dados:
```bash
docker compose up -d
```

### 2. Executar ingest√£o do PDF:
```bash
python src/ingest.py
```

### 3. Rodar o chat:
```bash
python3 src/chat.py
```

### 4. Rodar o chat com hist√≥rico:
```bash
python3 src/chat_with_history.py
```

## üí¨ Como Usar o CLI

### Interface do Chat
O sistema oferece uma interface de linha de comando interativa:

```
üîç Validando vari√°veis de ambiente...
üìã Vari√°veis configuradas:
  ‚úÖ GOOGLE_API_KEY = AIzaSyAU...
  ‚úÖ DATABASE_URL = postgresql+psycopg://postgres:postgres@localhost:5432/rag
  ‚úÖ PG_VECTOR_COLLECTION_NAME = pdf_chunks
  ‚úÖ PDF_PATH = document.pdf
‚úÖ Todas as vari√°veis de ambiente est√£o configuradas!

Fa√ßa sua pergunta (digite 'sair' para encerrar):

PERGUNTA DO USU√ÅRIO: me fala sobre o documento
RESPOSTA DO MODEL: O documento cont√©m uma lista de empresas, seus respectivos setores de atua√ß√£o, receita (em reais) e ano de funda√ß√£o. As empresas est√£o agrupadas por nomes que come√ßam com "√âpico", "√çmpar", "Tr√≠ade", "Vale", "Gr√£o", "Alian√ßa", "Esmeralda" e "Cobalto".

PERGUNTA DO USU√ÅRIO: qual empresa tem maior receita?
RESPOSTA DO MODEL: [Resposta baseada no conte√∫do do PDF]

PERGUNTA DO USU√ÅRIO: qual √© a capital da Fran√ßa?
RESPOSTA DO MODEL: N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta.

PERGUNTA DO USU√ÅRIO: sair
Encerrando chat.
```

### Comandos Dispon√≠veis
- **Qualquer pergunta**: O sistema busca no conte√∫do do PDF e responde
- **`sair`**, **`exit`**, **`quit`**: Encerra o chat

## ‚öôÔ∏è Requisitos do Projeto

### üìÑ Ingest√£o do PDF
- **Chunking**: PDF dividido em peda√ßos de 1000 caracteres com sobreposi√ß√£o de 150 ‚úÖ
- **Embeddings**: Cada chunk convertido em vetor usando `models/embedding-001` do Gemini ‚úÖ
- **Armazenamento**: Vetores salvos no PostgreSQL com extens√£o pgVector ‚úÖ

### üîç Busca Sem√¢ntica
- **Interface CLI**: Chat interativo no terminal ‚úÖ
- **Busca vetorial**: Encontra os 10 chunks mais relevantes (k=10) ‚úÖ
- **Gera√ß√£o de resposta**: LLM Gemini processa contexto e responde ‚úÖ
- **Valida√ß√£o de contexto**: Responde apenas com base no PDF ingerido ‚úÖ


## üîß Detalhes T√©cnicos

### Prompt Template Chat
O sistema utiliza o template disponibilizado pelo LangChain para garantir respostas baseadas apenas no contexto:

```python
PROMPT_TEMPLATE = """
CONTEXTO:
{contexto}

REGRAS:
- Responda somente com base no CONTEXTO.
- Se a informa√ß√£o n√£o estiver explicitamente no CONTEXTO, responda:
  "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."
- Nunca invente ou use conhecimento externo.
- Nunca produza opini√µes ou interpreta√ß√µes al√©m do que est√° escrito.

EXEMPLOS DE PERGUNTAS FORA DO CONTEXTO:
Pergunta: "Qual √© a capital da Fran√ßa?"
Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

Pergunta: "Quantos clientes temos em 2024?"
Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

Pergunta: "Voc√™ acha isso bom ou ruim?"
Resposta: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."

PERGUNTA DO USU√ÅRIO:
{pergunta}

RESPONDA A "PERGUNTA DO USU√ÅRIO"
"""
```

### Fluxo de Dados
1. **PDF ‚Üí Chunks**: Documento dividido em peda√ßos de 1000 caracteres
2. **Chunks ‚Üí Embeddings**: Vetoriza√ß√£o usando Gemini embeddings
3. **Embeddings ‚Üí PostgreSQL**: Armazenamento com pgVector
4. **Query ‚Üí Busca**: Similaridade sem√¢ntica nos vetores
5. **Contexto ‚Üí LLM**: Gera√ß√£o de resposta com Gemini


## üéÅ B√¥nus: Chat com Hist√≥rico Conversacional

> **Observa√ß√£o**: O chat com hist√≥rico conversacional foi implementado, entretanto, o prompt teve que ser alterado para adequar o comportamento do modelo, portanto, n√£o foi utilizado o prompt fornecido anteriormente.

### üí¨ Funcionalidade Extra
Al√©m do chat b√°sico, implementei um **chat com mem√≥ria conversacional**:

```bash
python3 src/chat_with_history.py
```

### ‚ú® Recursos Adicionais
- **Mem√≥ria de conversa**: Mant√©m contexto entre perguntas
- **Sess√µes independentes**: M√∫ltiplos usu√°rios simult√¢neos
- **Comandos especiais**:
  - `historico` - Ver hist√≥rico da conversa
  - `limpar` - Limpar mem√≥ria da sess√£o
  - `sair` - Encerrar chat
- **Interface aprimorada**: Emojis e formata√ß√£o visual


### üîÑ Exemplo de Uso Conversacional
```
ü§ñ Chat com Hist√≥rico - Sistema de Busca Sem√¢ntica
==================================================
üîç Validando vari√°veis de ambiente...
üìã Vari√°veis configuradas:
  ‚úÖ GOOGLE_API_KEY = AIzaSyAU...
  ‚úÖ DATABASE_URL = postgresql+psycopg://postgres:postgres@localhost:5432/rag
  ‚úÖ PG_VECTOR_COLLECTION_NAME = pdf_chunks
  ‚úÖ PDF_PATH = document.pdf
‚úÖ Todas as vari√°veis de ambiente est√£o configuradas!

Comandos especiais:
- 'sair', 'exit', 'quit': Encerrar chat
- 'limpar', 'clear': Limpar hist√≥rico
- 'historico', 'history': Ver hist√≥rico

FA√áA SUA PERGUNTA: me fala sobre a primeira empresa no contexto
RESPOSTA DO MODELO: A primeira empresa no contexto √© a √âpico Imobili√°ria Servi√ßos, com receita de R$ 12.800.650,98 e fundada em 1970.

FA√áA SUA PERGUNTA: qual √© o nome dela ?
RESPOSTA DO MODELO: √âpico Imobili√°ria Servi√ßos.

FA√áA SUA PERGUNTA: e da  Alfa Imobili√°ria S.A ?
RESPOSTA DO MODELO: A Alfa Imobili√°ria S.A. teve um faturamento de R$ 666.804.293,08 e foi fundada em 2012.

FA√áA SUA PERGUNTA: historico

Hist√≥rico da conversa:
1. VOC√ä: me fala sobre a primeira empresa no contexto
2. ASSISTENTE: A primeira empresa no contexto √© a √âpico Imobili√°ria Servi√ßos, com receita de R$ 12.800.650,98 e fun...
3. VOC√ä: qual √© o nome dela ?
4. ASSISTENTE: √âpico Imobili√°ria Servi√ßos.
5. VOC√ä: e da  Alfa Imobili√°ria S.A ?
6. ASSISTENTE: A Alfa Imobili√°ria S.A. teve um faturamento de R$ 666.804.293,08 e foi fundada em 2012.

```

### üèóÔ∏è Arquitetura Conversacional
- **`ConversationalSearchService`**: Classe principal com mem√≥ria
- **`InMemoryChatMessageHistory`**: Armazenamento de sess√µes
- **Message Trimming**: Mant√©m √∫ltimas 10 mensagens
- **Context Integration**: Combina busca sem√¢ntica + hist√≥rico

## üìä Status do Projeto

‚úÖ **Funcionalidades Implementadas**:
- Ingest√£o de PDF com chunking
- Armazenamento vetorial PostgreSQL + pgVector
- Busca sem√¢ntica com k=10
- Interface CLI interativa
- Valida√ß√£o de vari√°veis de ambiente
- Tipagem Python
- Classe SearchService organizada
- **üéÅ Chat com hist√≥rico conversacional**

‚úÖ **Evid√™ncias de testes**:
- Docker Compose com PostgreSQL
- Ingest√£o completa (67 chunks processados)
- Chat respondendo perguntas sobre o documento
- Rejei√ß√£o de perguntas fora do contexto
- **Chat conversacional mantendo contexto**

---
**üéì Desenvolvido para o desafio MBA Engenharia de Software com IA - Full Cycle**
